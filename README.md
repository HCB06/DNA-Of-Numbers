# DNA-Of-Numbers
Machine Learning Automation and My Unique Genetic Signature Formula

TEACHING THE MACHINE MACHINE LEARNING... (MACHINE LEARNING AUTOMATION PROGRAM)
- In this project we have a fully or semi automaticly train and testing software. This software basicly does Linear Regression models Classification Problems and some cases Polynomal Regression models.
This ai creation tool also ai ? (maybe)
Sometimes gives advice but it is rule-based software.
The most important point is that it uses my own error minimization method This method uses cases and like genetic algorithm process. The datas is 
individuals and cases individuals looking for spouses. Cases prefer the data that suits them best. The advantage is more controllable options and speed.
The classical method actually finds it faster, but since we do not know what the number we are looking for is, this turns into a trial and error process, it turns into a search for the most appropriate iteration, and this brings slowness in terms of speed.
What I am trying to say is that since we do not know the exact number we are looking for, we learn that this loop process should be at that value by specifying the loop process as exiting the loop when the error starts to increase (an incredible iteration occurred). We set an iteration limit and train (again, the process takes longer)
So, in the classical method, the work takes longer.
To prevent this, I created a new error minimization and automation process without using derivatives.
However, I also brought new naming such as alpha beta and case, alpha and beta here do not represent the alpha and beta you see elsewhere. In my method, alpha and beta represent the initial values â€‹â€‹of the theta parameters, I apply a drop point optimization.

- But its most remarkable feature is creating a genetic map of the data (datasets) while performing its task. In other words, every dataset that enters the algorithm leaves behind its unique DNA. This means that when the data needs to be retrained, if the machine finds a match during the training process...ðŸ‘‡

- We can store this information in a log. By looking at this matching, we can instantly understand which previously trained data your newly added training data aligns with, greatly enhancing the speed and overall privacy of updated training. In classical methods, it remains the almost same for every dataset since it relies on a fixed formula.
In the DNA of this dataset, each point represents the connection points of a nucleotide, and the nucleotides serve as vertical representations of these connection points. Each of these nucleotides harbors different forms of gradient descents. In essence, as I explained in my previous video: https://youtu.be/fAff2dqv--M?si=mDdUuhheArVpb0kT
if there is a value lower than the minimum found in another CASE, that nucleotide connects to the other, forming a pattern (creating a path). I prefer to this path as the DNA of the dataset. In the DNA strands of living organisms, nucleotides were connected based on specific base pairs. In my program, for maintenance purposes, to train machine learning information, this path is kept in a log, and if newly added data matches this path, it automatically inherits these nucleotides to the offspring. This can be done instantly with the help of a control software, without the need for unnecessary iterations.

In classical formulas, descent is typically straightforward, but in my formula, it progresses like a DNA helix, and I will leave the answers to all the questions below. I explain it in the YouTube link.

However, in classical formulas, such a unique trace is never encountered; usually, a single type of optimization descent is observed.

NOTE: This is not a simple genetic searching way. (I am aware that the genetic algorithm encompasses various dynamics, and I want to emphasize that I have not used all of them because I did not need to. Remember: This Is Not Just An Optimization Method; It's An Optimization Formula. I Have Created Different Structures Based On Cases, Completely Revamping Calculus-based Formulas Into Highly Potential Structures That You Can Be Certain Have Never Been Seen Elsewhere.)

In addition, I'm trying to create a new concept called 'Conscious Learning' alongside supervised, unsupervised, and reinforcement learning. My work is ongoing. So, why 'Conscious Learning'? Because machines' learning processes now progress with data for all types, and we can liken data to memory. We have already made this memory more recallable, just like artificial neural networks rebuilding structures from scratch in a single go in their learning processes. What I want to do is develop a learning approach where a machine can decide for itself what to learn and what not to learn, dynamically control a consciousness structure, and continuously learn. This can be achieved through what I refer to as 'DNA memory' in this article (DNA log system). You can watch the video where I discuss this concept in Turkish: https://youtu.be/RtWOPxQdhxQ?si=A23PmMQVR1JoMkI3

If we return to the algorithm...
Below, I emphasize that each nucleotide actually represents different forms of gradient descents. These gradients connect to each other, forming a continuous descent, resulting in a grand gradient. In the DNA structure, for one nucleotide to connect to another, it depends on which base content is predominant. The ratios of bases like adenine and thymine determine the continuation of a nucleotide. In my algorithm, I refer to these bases as 'Cases.' The parameters we are looking for are numbers, and each number corresponds to a point. In essence, we are seeking the hyperparameter within a nucleotide that gives us the lowest value based on our defined criterion (error value). For example, let's assume we are looking for a nucleotide with a high adenine content; in my algorithm, the equivalent of adenine is labeled as 'case3.' In this case, the descent in the DNA is long because there is a high adenine ratio, and in my algorithm, it corresponds to a longer duration in 'case3.' As you progress through the video, you'll notice that these two align, but in the third case, it changes direction. This change in direction signifies a move to the next 'case.' However, as you can see, we continue to approach the correct solution, meaning we are descending. After these operations, the nucleotide with the most accurate base content becomes the one we are looking for. And we also identify the hyperparameter that finds this nucleotide in a single descent.

We are searching for a nucleotide sequence, which is essentially a gradient sequence.



For more informaiton about my method watch my this videos:
1.) https://youtu.be/-aYUKgH3JiA?si=pJU8Zi9upLwLhWpJ
2.) https://youtu.be/fAff2dqv--M?si=HtxjCvtP-4UbIxz_
3.) https://youtu.be/DoSkxyvhqhU?si=mZldCiNdviQ3dWKG
4.) https://youtu.be/3o7NZdSR2Pc?si=kjH52cLmlYLkeBWM
5.) https://youtu.be/MT3B49X-WgY?si=K-3A51C0uP8Bik8N

End of all watch my this video (Turkish): https://youtu.be/RtWOPxQdhxQ?si=A23PmMQVR1JoMkI3
